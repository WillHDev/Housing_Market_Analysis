{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fab768",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/data_documentation.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36m_get_lines\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/data_documentation.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     63\u001b[39m         lines = file.readlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/data_documentation.txt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 375\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.max(np.abs(y_pred - y_true))\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# This code is executed once during import time and\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# populates all the \"constants\" directly or indirectly.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m _extract_meta_data(\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    376\u001b[39m _populate_dicts_and_lists()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36m_get_lines\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     66\u001b[39m response = requests.get(\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://www.amstat.org/publications\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/jse/v19n3/decock/DataDocumentation.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m )\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Cache the retrieved file.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/data_documentation.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m     72\u001b[39m     file.write(response.text)\n\u001b[32m     73\u001b[39m lines = response.text.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/data_documentation.txt'"
     ]
    }
   ],
   "source": [
    "# \"\"\"Description of the Ames Housing dataset.\n",
    "\n",
    "# This module uses the information available on the publication homepage and\n",
    "# defines a nested dictionary `ALL_COLUMNS` that can be used to decode the data\n",
    "# in the accompanying Excel file. For convenience, `ALL_VARIABLES` provides a\n",
    "# list of only the column names.\n",
    "\n",
    "# Furthermore, six helper dictionaries `CONTINUOUS_COLUMNS`, `DISCRETE_COLUMNS`,\n",
    "# `NUMERIC_COLUMNS`, `NOMINAL_COLUMNS`, `ORDINAL_COLUMNS`, and `LABEL_COLUMNS`\n",
    "# are defined that provide just the subset of the columns with the corresponding\n",
    "# data types. Note that the numeric dictionary unifies the continuous and\n",
    "# discrete data columns while the label dictionary unifies the nominal and\n",
    "# ordinal columns. For each of the six dictionaries, a list of the actual column\n",
    "# names is created with the same name and the suffix \"_VARIABLES\" instead of\n",
    "# \"_COLUMNS\", e.g., \"CONTINUOUS_VARIABLES\" instead of \"CONTINUOUS_COLUMNS\".\n",
    "\n",
    "# Lastly, the INDEX_COLUMNS and LABEL_TYPES list can be used to refer to the\n",
    "# actual values in a more readable way.\n",
    "\n",
    "# Source:\n",
    "#     https://www.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt\n",
    "\n",
    "# Implementation Note:\n",
    "#     This file defines the \"constants\" it exports dynamically. This is a bit\n",
    "#     advanced but intentional!\n",
    "# \"\"\"\n",
    "\n",
    "# import re\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# import tabulate\n",
    "\n",
    "\n",
    "# FACTOR_VARIABLES = []\n",
    "# INDEX_COLUMNS = [\"Order\", \"PID\"]\n",
    "# LABEL_TYPES = [\"nominal\", \"ordinal\"]\n",
    "# TARGET_VARIABLES = [\"SalePrice\"]\n",
    "# # Note that these dictionaries and lists are not actually constants but\n",
    "# # filled in during import time which makes them \"near\"-constant.\n",
    "# ALL_COLUMNS = {}\n",
    "# ALL_VARIABLES = []\n",
    "# CONTINUOUS_COLUMNS = {}\n",
    "# CONTINUOUS_VARIABLES = []\n",
    "# DISCRETE_COLUMNS = {}\n",
    "# DISCRETE_VARIABLES = []\n",
    "# NUMERIC_COLUMNS = {}\n",
    "# NUMERIC_VARIABLES = []\n",
    "# NOMINAL_COLUMNS = {}\n",
    "# NOMINAL_VARIABLES = []\n",
    "# ORDINAL_COLUMNS = {}\n",
    "# ORDINAL_VARIABLES = []\n",
    "# LABEL_COLUMNS = {}\n",
    "# LABEL_VARIABLES = []\n",
    "\n",
    "\n",
    "# def _get_lines():\n",
    "#     \"\"\"Obtain the non-empty lines of the data description file.\"\"\"\n",
    "#     # Read cached data file.\n",
    "#     try:\n",
    "#         with open(\"data/data_documentation.txt\", \"r\") as file:\n",
    "#             lines = file.readlines()\n",
    "#     # If there is no cached file, obtain in from the original source.\n",
    "#     except FileNotFoundError:\n",
    "#         response = requests.get(\n",
    "#             \"https://www.amstat.org/publications\"\n",
    "#             \"/jse/v19n3/decock/DataDocumentation.txt\"\n",
    "#         )\n",
    "#         # Cache the retrieved file.\n",
    "#         with open(\"data/data_documentation.txt\", \"w\") as file:\n",
    "#             file.write(response.text)\n",
    "#         lines = response.text.split(\"\\r\\n\")\n",
    "#     # Remove header, footer, and empty lines.\n",
    "#     lines = [x.replace(\"  \", \" \").strip() for x in lines[13:545]]\n",
    "#     lines = [x for x in lines if x != \"\"]\n",
    "\n",
    "#     return lines\n",
    "\n",
    "\n",
    "# def _extract_meta_data(lines):\n",
    "#     \"\"\"Extract variables and realizations for a line.\n",
    "\n",
    "#     This function parses the lines from the data documentation file and\n",
    "#     writes the results into the global dictionary ALL_COLUMNS that is exported\n",
    "#     by this module.\n",
    "\n",
    "#     A line can be a variable consisting of:\n",
    "#         - the name of the variable / column,\n",
    "#         - the variable's type (continuous, discrete, nominal, or ordinal), and\n",
    "#         - a text description of the variable.\n",
    "\n",
    "#     A line can also be a realization of a label column consisting of:\n",
    "#         - the encoding,\n",
    "#         - and the description.\n",
    "\n",
    "#     Implementation note:\n",
    "#         As the lines come in order, the \"elif\" condition below correctly refers\n",
    "#         to the last line representing a variable.\n",
    "#     \"\"\"\n",
    "#     variable = re.compile(r\"^(.*)(?:[\\s]+)\\(([\\w]*)\\)(?:\\t)?: (.*)$\")\n",
    "#     realization = re.compile(r\"^(.*)\\t(.*)$\")\n",
    "#     # The two ID columns and the target variable \"SalePrice\"\n",
    "#     # are not put into the helper dicts / lists as they are\n",
    "#     # treated seperately in the modelling anyways.\n",
    "#     non_feature_columns = INDEX_COLUMNS + TARGET_VARIABLES\n",
    "\n",
    "#     for line in lines:\n",
    "#         # Process the next variable in the list.\n",
    "#         match = variable.match(line)\n",
    "#         if match:\n",
    "#             name, type_, description = match.groups()\n",
    "#             # Skip the non-feature columns (that are always non-label columns).\n",
    "#             if name in non_feature_columns:\n",
    "#                 continue\n",
    "#             type_ = type_.lower()\n",
    "#             # Create an entry for the next variable in the list.\n",
    "#             ALL_COLUMNS[name] = {\"type\": type_, \"description\": description}\n",
    "#             # Only if the variable is a label type, a lookup table is needed.\n",
    "#             if type_ in LABEL_TYPES:\n",
    "#                 ALL_COLUMNS[name].update({\"lookups\": {}})\n",
    "#             # Ordinal variables also store the order of their realizations\n",
    "#             # exactly as defined in the data description file.\n",
    "#             if type_ == \"ordinal\":\n",
    "#                 ALL_COLUMNS[name].update({\"order\": []})\n",
    "#         # Add label realizations to a previously found label variable.\n",
    "#         elif type_ in LABEL_TYPES:\n",
    "#             match = realization.match(line)\n",
    "#             code, description = match.groups()\n",
    "#             code = code.strip()\n",
    "#             ALL_COLUMNS[name][\"lookups\"][code] = description\n",
    "#             if type_ == \"ordinal\":\n",
    "#                 ALL_COLUMNS[name][\"order\"].append(code)\n",
    "\n",
    "\n",
    "# def _populate_dicts_and_lists():\n",
    "#     \"\"\"Populate all \"secondary\" dictionaries and lists.\n",
    "\n",
    "#     The ALL_COLUMNS dictionary is the \"main\" dictionary and all other global\n",
    "#     dictionaries and lists are considered derived from it and thus considered\n",
    "#     \"secondary\".\n",
    "#     \"\"\"\n",
    "#     # The global data structures are not re-assigned to so as to keep all\n",
    "#     # references in the Jupyter notebooks alive. Instead, they are emptied\n",
    "#     # and re-filled.\n",
    "#     ALL_VARIABLES[:] = sorted(ALL_COLUMNS)\n",
    "#     CONTINUOUS_COLUMNS.clear()\n",
    "#     CONTINUOUS_COLUMNS.update(\n",
    "#         {\n",
    "#             key: value\n",
    "#             for (key, value) in ALL_COLUMNS.items()\n",
    "#             if value[\"type\"] == \"continuous\"\n",
    "#         }\n",
    "#     )\n",
    "#     CONTINUOUS_VARIABLES[:] = sorted(CONTINUOUS_COLUMNS)\n",
    "#     DISCRETE_COLUMNS.clear()\n",
    "#     DISCRETE_COLUMNS.update(\n",
    "#         {\n",
    "#             key: value\n",
    "#             for (key, value) in ALL_COLUMNS.items()\n",
    "#             if value[\"type\"] == \"discrete\"\n",
    "#         }\n",
    "#     )\n",
    "#     DISCRETE_VARIABLES[:] = sorted(DISCRETE_COLUMNS)\n",
    "#     FACTOR_VARIABLES[:] = [\n",
    "#         key\n",
    "#         for (key, value) in ALL_COLUMNS.items()\n",
    "#         if value[\"type\"] == \"factor\"\n",
    "#     ]\n",
    "#     NUMERIC_COLUMNS.clear()\n",
    "#     NUMERIC_COLUMNS.update({**CONTINUOUS_COLUMNS, **DISCRETE_COLUMNS})\n",
    "#     NUMERIC_VARIABLES[:] = sorted(NUMERIC_COLUMNS)\n",
    "#     NOMINAL_COLUMNS.clear()\n",
    "#     NOMINAL_COLUMNS.update(\n",
    "#         {\n",
    "#             key: value\n",
    "#             for (key, value) in ALL_COLUMNS.items()\n",
    "#             if value[\"type\"] == \"nominal\"\n",
    "#         }\n",
    "#     )\n",
    "#     NOMINAL_VARIABLES[:] = sorted(NOMINAL_COLUMNS)\n",
    "#     ORDINAL_COLUMNS.clear()\n",
    "#     ORDINAL_COLUMNS.update(\n",
    "#         {\n",
    "#             key: value\n",
    "#             for (key, value) in ALL_COLUMNS.items()\n",
    "#             if value[\"type\"] == \"ordinal\"\n",
    "#         }\n",
    "#     )\n",
    "#     ORDINAL_VARIABLES[:] = sorted(ORDINAL_COLUMNS)\n",
    "#     LABEL_COLUMNS.clear()\n",
    "#     LABEL_COLUMNS.update({**NOMINAL_COLUMNS, **ORDINAL_COLUMNS})\n",
    "#     LABEL_VARIABLES[:] = sorted(LABEL_COLUMNS)\n",
    "\n",
    "\n",
    "# def _rename_column(old_name, new_name):\n",
    "#     \"\"\"Change the name of a column.\"\"\"\n",
    "#     ALL_COLUMNS[new_name] = ALL_COLUMNS[old_name]\n",
    "#     del ALL_COLUMNS[old_name]\n",
    "\n",
    "\n",
    "# def correct_column_names(data_columns, *, repopulate=True):\n",
    "#     \"\"\"Cross-check the column names between data and description file.\n",
    "\n",
    "#     In rare cases, the variable name in the data description file was slightly\n",
    "#     changed, i.e., a dash or a space needs to be removed.\n",
    "\n",
    "#     This function adjusts the keys in all the dictionaries and lists.\n",
    "#     \"\"\"\n",
    "#     for desc_column in ALL_VARIABLES:\n",
    "#         if desc_column not in data_columns:\n",
    "#             for data_column in data_columns:\n",
    "#                 # Column name was truncated in description file.\n",
    "#                 if data_column.startswith(desc_column):\n",
    "#                     _rename_column(desc_column, data_column)\n",
    "#                     break\n",
    "#                 # Spaces between words in Excel were removed.\n",
    "#                 adj_data_column = data_column.replace(\" \", \"\")\n",
    "#                 if adj_data_column == desc_column:\n",
    "#                     _rename_column(desc_column, data_column)\n",
    "#                     break\n",
    "#                 # Spaces between words in description file were removed.\n",
    "#                 adj_desc_column = desc_column.replace(\" \", \"\")\n",
    "#                 if adj_data_column == adj_desc_column:\n",
    "#                     _rename_column(desc_column, data_column)\n",
    "#                     break\n",
    "#                 # Dashes in description file were removed.\n",
    "#                 adj_desc_column = desc_column.replace(\"-\", \"\")\n",
    "#                 if data_column == adj_desc_column:\n",
    "#                     _rename_column(desc_column, data_column)\n",
    "#                     break\n",
    "#     # Propagate the change to all \"secondary\" dictionaries and lists.\n",
    "#     if repopulate:\n",
    "#         _populate_dicts_and_lists()\n",
    "\n",
    "\n",
    "# def update_column_descriptions(columns_to_be_kept, *, correct_columns=False):\n",
    "#     \"\"\"Remove discarded columns for all the module's exported data structures.\n",
    "\n",
    "#     After dropping some columns from the DataFrame, these removals must be\n",
    "#     propagated to the helper data structures defined in this module.\n",
    "#     \"\"\"\n",
    "#     if correct_columns:\n",
    "#         correct_column_names(columns_to_be_kept, repopulate=False)\n",
    "#     columns_to_be_removed = list(set(ALL_COLUMNS) - set(columns_to_be_kept))\n",
    "#     for column in columns_to_be_removed:\n",
    "#         del ALL_COLUMNS[column]\n",
    "#     # Propagate the change to all \"secondary\" dictionaries and lists.\n",
    "#     _populate_dicts_and_lists()\n",
    "\n",
    "\n",
    "# def print_column_list(subset=None):\n",
    "#     \"\"\"Print (a subset of) the data's column headers.\n",
    "\n",
    "#     Note that this function is built to handle both *_COLUMNS dicts and\n",
    "#     *_VARIABLES lists.\n",
    "#     \"\"\"\n",
    "#     if subset is None:\n",
    "#         subset = ALL_VARIABLES\n",
    "#     else:\n",
    "#         subset = set(subset)\n",
    "#         # Handle variables withoutdescription seperately.\n",
    "#         without_desc = subset - set(ALL_VARIABLES)\n",
    "#         subset -= without_desc\n",
    "#     columns = [(c, ALL_COLUMNS[c][\"description\"]) for c in subset]\n",
    "#     if without_desc:\n",
    "#         for column in sorted(without_desc):\n",
    "#             columns.append((column, \"\"))\n",
    "#     print(tabulate.tabulate(sorted(columns), tablefmt=\"plain\"))\n",
    "\n",
    "\n",
    "# def load_clean_data(path=None):\n",
    "#     \"\"\"Return the clean project data as a pandas DataFrame.\n",
    "\n",
    "#     This utility function ensures that each column is cast to its correct type.\n",
    "\n",
    "#     It takes an optional path argument to a clean CSV file (defaults to\n",
    "#     \"data/data_clean.csv\").\n",
    "\n",
    "#     The target variables are always included as the last columns.\n",
    "\n",
    "#     Implementation Notes:\n",
    "\n",
    "#     One caveat is that all columns need to be casted as generic object type\n",
    "#     first, then the column names in the global dicts and lists are updated to\n",
    "#     reflect the slightly different column names (between data and description\n",
    "#     files), after which only the numeric columns can be casted correctly.\n",
    "\n",
    "#     Another difficulty is that some values, e.g., \"NA\" strings are cast as\n",
    "#     np.NaN / None by pandas although they represent actual label values.\n",
    "#     \"\"\"\n",
    "#     # pragma pylint:disable=invalid-name\n",
    "#     df = pd.read_csv(\n",
    "#         \"data/data_clean.csv\" if path is None else path,\n",
    "#         index_col=INDEX_COLUMNS,\n",
    "#         dtype=object,\n",
    "#         na_values=\"\",  # There are no missing values in the clean data file.\n",
    "#         keep_default_na=False,  # \"NA\" strings are casted as actual values.\n",
    "#     )\n",
    "#     # Remove columns that are in the description but not in the data file.\n",
    "#     update_column_descriptions(df.columns, correct_columns=True)\n",
    "#     # Cast the numeric types correctly.\n",
    "#     for column in CONTINUOUS_VARIABLES + TARGET_VARIABLES:\n",
    "#         df[column] = df[column].astype(float)\n",
    "#     for column in DISCRETE_VARIABLES:\n",
    "#         df[column] = df[column].astype(int)\n",
    "#     # Cast the label types as Categoricals.\n",
    "#     for column, mapping in NOMINAL_COLUMNS.items():\n",
    "#         labels = pd.api.types.CategoricalDtype(\n",
    "#             mapping[\"lookups\"].keys(), ordered=False\n",
    "#         )\n",
    "#         df[column] = df[column].astype(labels)\n",
    "#     for column, mapping in ORDINAL_COLUMNS.items():\n",
    "#         labels = pd.api.types.CategoricalDtype(\n",
    "#             reversed(mapping[\"order\"]), ordered=True\n",
    "#         )\n",
    "#         df[column] = df[column].astype(labels)\n",
    "#     # After the raw data cleaning, several derived variables were created.\n",
    "#     derived_columns = set(df.columns) - set(ALL_VARIABLES + TARGET_VARIABLES)\n",
    "#     if derived_columns:\n",
    "#         for column in derived_columns:\n",
    "#             # Check if the derived variable is a target variable.\n",
    "#             for target in TARGET_VARIABLES[:]:\n",
    "#                 if column.startswith(target):\n",
    "#                     df[column] = df[column].astype(float)\n",
    "#                     TARGET_VARIABLES.append(column)\n",
    "#                     break\n",
    "#             else:\n",
    "#                 df[column] = df[column].astype(float)\n",
    "#                 is_int = (df[column] == df[column].astype(int)).all()\n",
    "#                 n_unique = len(df[column].unique())\n",
    "#                 if is_int & (n_unique == 2):\n",
    "#                     df[column] = df[column].astype(int)\n",
    "#                     type_ = \"factor\"\n",
    "#                 elif is_int & (n_unique < 150):\n",
    "#                     df[column] = df[column].astype(int)\n",
    "#                     type_ = \"discrete\"\n",
    "#                 else:\n",
    "#                     df[column] = df[column].astype(float)\n",
    "#                     type_ = \"continuous\"\n",
    "#                 ALL_COLUMNS[column] = {\n",
    "#                     \"type\": type_,\n",
    "#                     \"description\": \"derived variable\",\n",
    "#                 }\n",
    "#         _populate_dicts_and_lists()\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def encode_ordinals(df):\n",
    "#     \"\"\"Replace ordinal columns' labels with integer codes.\"\"\"\n",
    "#     # pragma pylint:disable=invalid-name\n",
    "#     df = df.copy()\n",
    "#     for column in df.columns:\n",
    "#         if column in ORDINAL_VARIABLES:\n",
    "#             df[column] = df[column].cat.codes.astype(int)\n",
    "#     return df\n",
    "\n",
    "\n",
    "# def bias_score(y_true, y_pred):\n",
    "#     \"\"\"Determine the bias of a prediction.\"\"\"\n",
    "#     assert y_true.shape == y_pred.shape\n",
    "#     assert y_true.ndim == 1\n",
    "#     return np.mean(y_pred - y_true)\n",
    "\n",
    "\n",
    "# def max_deviation(y_true, y_pred):\n",
    "#     \"\"\"Determine the maximum deviation of a prediction.\"\"\"\n",
    "#     assert y_true.shape == y_pred.shape\n",
    "#     assert y_true.ndim == 1\n",
    "#     return np.max(np.abs(y_pred - y_true))\n",
    "\n",
    "\n",
    "# # This code is executed once during import time and\n",
    "# # populates all the \"constants\" directly or indirectly.\n",
    "# _extract_meta_data(_get_lines())\n",
    "# _populate_dicts_and_lists()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
